---
title: "Dual Reverse Attention Networks for Person RE-IDentification"
published: true
---
<style>

p.small {
  font-variant: normal;
  font-size: 20px;
}

a:link, a:visited {
  display: block;
  color: #000;
  text-decoration: none;
  display: inline-block;
  text-align: center;
  background: white;

  /* "to left" / "to right" - affects initial color */
  background: linear-gradient(to left, white 50%, black 50%) right;
  background-size: 200%;
  transition: .5s ease-out;
}

a:hover, a:active {
  background-position: left;
  /* background-color: #555; */
  color: white;
  -webkit-transform: scale(1);
  -ms-transform: scale(1);
  transform: scale(1);
}
h2.thicker_head{
  font: 35px Arial, sans-serif;
  font-weight: 500;
  margin-top: 40px;
}
h3.thicker_verdict{
  font: 35px Arial, sans-serif;
  font-weight: 500;
  margin-top: 20px;
}

h1.thicker_head{
  font: 45px Arial, sans-serif;
  font-weight: 800;
}
.img-container {
  position: relative;
  top: 0;
  left: 0;
  display:block;
  margin: auto;
}
.top {

}
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
</style>

<!-- <metadata -->

<h1 class= "thicker_head {% if site.style == 'dark' %}class="text-white"{% endif %}" margin-top="15px">What ?</h1>
<p class="small">
Person Re-ID using a dual reverse attention based module. Creates hard examples by transforming features using channel and spatial reverse attention process. </p>

<h1 class= "thicker_head {% if site.style == 'dark' %}class="text-white"{% endif %}" margin-top="15px">Why ?</h1>
<p class="small">
Inability to generalise on hard examples(occlusion, light variances etc.). Mining / synthetic data generation is expensive and suffers on convergence problem. </p>

<h1 class= "thicker_head {% if site.style == 'dark' %}class="text-white"{% endif %}" margin-top="15px">How ?</h1>
<p class="small">
The authors introduce DRANet (Dual Reverse Attention Networks) to generate hard examples in convolutional feature space.</p>

<img src="/images/DRAN_PID/arch.png" width="512" height="256" border="1px" alt="arch" class="center">
<!-- <img src="/images/fellow_exp/emotion_classes.png" width="256" height="256" border="1px" class="center"> -->

<!-- <div align="center" class="img-container">

    <img src="/images/DRAN_PD/arch.png" alt="synthetic" width="256" height="256" >
    <figcaption>DRANet architecture</figcaption>
</div> -->

<p class="small">
The input image is fed to ResNet50 and features maps are divided into three parts (<b>Why ?</b>). Each partial feature sequentially goes through CIC (Channel Information Capture) and SIC (Spatial Information Capture).</p>

<h2 class= "thicker_head {% if site.style == 'dark' %}class="text-white"{% endif %}" margin-top="15px">Dual Reverse Attention</h2>

<div align="center" class="img-container">
    <img src="/images/DRAN_PID/rcam.png" alt="rcam" width="512" height="256" >
    <figcaption>Channel based reverse Attention</figcaption>

    <img src="/images/DRAN_PID/rsam.png" alt="rsam" width="512" height="256" >
    <figcaption>Spatial based reverse Attention</figcaption>
</div>
<p class="small">
CIC provides a mask 'm' to suppress informative features and generate channel based hard examples.
<!-- The authors use a squeeze function (FCL) to extract descriptive features for each channel and an excitation function to reflect relative importance of each channel (m) . Later they are combined using channel-wise multiplication for informative features.  <b> (B) </b> -->
Similarly, SIC provides a mask, but <b>'only for'</b> corresponding person identity <i>'n'</i> (Only available during training).</p>

<h2 class= "thicker_head {% if site.style == 'dark' %}class="text-white"{% endif %}" margin-top="15px">Implementation</h2>
<p class="small">
All <b>three</b> softmax losses are combined during training process. During testing, only B and SIC is possible (since person identity is available for reverse attention spatially). Encodings from both features maps after GAP are concatenated (2048 - 1024*2) for predictions.</p>

<h2 class= "thicker_head {% if site.style == 'dark' %}class="text-white"{% endif %}" margin-top="15px">Notes</h2>
<p class="small">
The model improves mAP on three dataset Market-150, DukeMTMC-reID, CUHK03 by 2, 3 & 5 %. But performs almost similar for rank 1, 5, and 10 accuracy.</p>
